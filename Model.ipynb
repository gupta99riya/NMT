{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "23L4NpUP5_TC"
      },
      "source": [
        "# Neural Machine Translation\n",
        "\n",
        "###  Predicting Machine Redable dates from Human readable date\n",
        "\n",
        "For instance, there are variety of possible human readable formats (e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"), this network will convert that into standardized, machine readable dates (e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"). The model is run on Google colab with GPU. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mMmUrJJ28C1p"
      },
      "source": [
        "## Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoG8zfUogjYs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "fa4fef15-7de5-4ed3-a1b1-673111e26701"
      },
      "source": [
        "pip install Faker"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Faker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/64/079c012bed8bc416fa0d136ceff86a7d25e765f45d7a0a97e0b803b1d506/Faker-4.0.3-py3-none-any.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 26.4MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 31.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 18.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 14.3MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 13.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 12.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 12.2MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 12.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 12.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 12.5MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 12.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 133kB 12.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 143kB 12.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 153kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 163kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 174kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 184kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 194kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 204kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 215kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 225kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 235kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 245kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 256kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 266kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 276kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 286kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 296kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 307kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 317kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 327kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 337kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 348kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 358kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 368kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 378kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 389kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 399kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 409kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 419kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 430kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 440kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 450kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 460kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 471kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 481kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 491kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 501kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 512kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 522kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 532kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 542kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 552kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 563kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 573kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 583kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 593kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 604kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 614kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 624kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 634kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 645kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 655kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 665kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 675kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 686kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 696kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 706kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 716kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 727kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 737kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 747kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 757kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 768kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 778kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 788kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 798kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 808kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 819kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 829kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 839kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 849kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 860kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 870kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 880kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 890kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 901kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 911kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 921kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 931kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 942kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 952kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 962kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 972kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 983kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 993kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from Faker) (1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from Faker) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.4->Faker) (1.12.0)\n",
            "Installing collected packages: Faker\n",
            "Successfully installed Faker-4.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sgGsvqiTc5Ty",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ad81403-864d-478b-87ba-e1c6f9ee0233"
      },
      "source": [
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "#pip install Faker\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8FekgqIu__pm"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "brVDnmlfeJDF"
      },
      "source": [
        "Data used for this analysis is from [Faker](https://pypi.org/project/Faker/), a Python package that generates fake data for you. It can generate address, name, text, date etc..\n",
        "\n",
        "Following are the steps used for data preparation:\n",
        "1. Fetching data from Faker and creating human readable and machine readable dates.\n",
        "2. Creating a list of both the dates and dictionary of human vocab and machine vocab by assigning index to each of the unique character/integer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DRmKtjQ--Vi2",
        "colab": {}
      },
      "source": [
        "# Formats consist of all possible formats of date \n",
        "FORMATS = ['short','medium','medium','medium','long','long','long','long','long','full','full','full','d MMM YYY',\n",
        "           'd MMMM YYY','d MMMM YYY','d MMMM YYY','d MMMM YYY','d MMMM YYY','dd/MM/YYY','EE d, MMM YYY','EEEE d, MMMM YYY']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qLx3u45n-EEk",
        "colab": {}
      },
      "source": [
        "def random_date():\n",
        "    data = Faker().date_object()\n",
        "\n",
        "    try:\n",
        "        date = format_date(data, format=random.choice(FORMATS), locale='en')\n",
        "        human_date = date.lower().replace(',', '')\n",
        "        machine_date = data.isoformat()\n",
        "\n",
        "    except AttributeError as e:\n",
        "        return None, None, None\n",
        "\n",
        "    return human_date, machine_date, data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LSDuv-_u9W7Y",
        "colab": {}
      },
      "source": [
        "\n",
        "def load_dataset(size):\n",
        "    human_vocab = set()\n",
        "    machine_vocab = set()\n",
        "    dataset = []\n",
        "    \n",
        "    '''\n",
        "    size: Number of instances in the data used for the analysis\n",
        "    dataset: List of two groups of dates (human readable date, machine readable date)\n",
        "    human_vocab: mapping of all characters used in human readable dates to dictionary index values\n",
        "    machine_vocab: mapping of all characters used in machine-readable dates to dictionary index values ​​(not necessarily identical to the index of human_vocab)\n",
        "    inv_machine_vocab: flip map of machine_vocab, mapping from index to character\n",
        "    '''\n",
        "    for i in tqdm(range(size)):\n",
        "        h, size, _ = random_date()\n",
        "        if h is not None:\n",
        "            dataset.append((h, size))\n",
        "            human_vocab.update(tuple(h))\n",
        "            machine_vocab.update(tuple(size))\n",
        "    \n",
        "    # Two special characters are added: <unk> for unknown characters in case if the word/character is not in the vocabulary, and <pad> to add padding at the end of the final sequence\n",
        "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], list(range(len(human_vocab) + 2))))\n",
        "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
        "    machine = {v: k for k, v in inv_machine.items()}\n",
        " \n",
        "    return dataset, human, machine, inv_machine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "snHOItDseHmC",
        "outputId": "8c0a6dce-858d-486d-8a4b-24bb0a469465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "size = 30000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [06:11<00:00, 80.84it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zyYkS2c2l_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle \n",
        "f = open(\"human_vocab.pkl\",\"wb\")\n",
        "pickle.dump(human_vocab,f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"inv_machine_vocab.pkl\",\"wb\")\n",
        "pickle.dump(inv_machine_vocab,f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NcOg9S2AENuQ"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Here data will be processed by getting indexes corresponding to each of the input date using dictionaries created in previous step. Also, the length of each row in the process data will be of same size using padding. Later, one hot encoding is created for the indexes data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "esAo8hWzA05t",
        "colab": {}
      },
      "source": [
        "def string_to_int(string, length, vocab):\n",
        "    string = string.lower()\n",
        "    string = string.replace(',','')\n",
        "    \n",
        "    if len(string) > length:\n",
        "        string = string[:length]\n",
        "        \n",
        "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
        "    \n",
        "    if len(string) < length:\n",
        "        rep += [vocab['<pad>']] * (length - len(string))\n",
        "    \n",
        "    return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x06Sr3AkAnOh",
        "colab": {}
      },
      "source": [
        "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
        "    X, Y = zip(*dataset)\n",
        "    \n",
        "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
        "    Y = np.array([string_to_int(t, Ty, machine_vocab) for t in Y])\n",
        "    \n",
        "    X_1 = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
        "    Y_1 = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
        "\n",
        "    return X, Y,X_1,Y_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x2VoFtQ8fZTX",
        "colab": {}
      },
      "source": [
        "Tx=30 # Timestep/Length of input date\n",
        "Ty=10 # Timestep/Length of output date\n",
        "X,Y,X_1,Y_1 = preprocess_data(dataset,human_vocab,machine_vocab,Tx,Ty)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WqJ80DvvJOrX"
      },
      "source": [
        "## Sequence to Sequence Model using Attention Mechanism "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MhiXEKrZEy0y"
      },
      "source": [
        "Sequence to Sequence model uses an Bidirectional RNN arcitecture so that the model will have forward and backward information (in the form of activations) at every step so that the model will learn from previous as well later instances. Attention mechanism is used by the network to identify how much focus to be made for one partcular time step as information from multiple time step will be folating into the prediction for that instance. \n",
        "\n",
        "A small neural network is used to identify the attention weights using the activation and LSTM layer of the decoder vector. Context is calculated using the attention weights and activated factors which will be passed as the information to the Post-attention LSTM layer of decoder. \n",
        "\n",
        "<center>        \n",
        "<img src=\"attn_mechanism.png\" style=\"width:500;height:500px;\"> \n",
        "</center>\n",
        "<caption><center> **Figure 1**: Atttention Mechanism</center></caption>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rCtBH-UaEOrm",
        "colab": {}
      },
      "source": [
        "# Different layers required for Attention calculation \n",
        "\n",
        "repeator     = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1      = Dense(10,activation='tanh')\n",
        "densor2      = Dense(1,activation='relu')\n",
        "activator    = Activation('softmax',name=\"attention_weights\")\n",
        "dotor        = Dot(axes=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GR1VEiaBIFwu",
        "colab": {}
      },
      "source": [
        "# Forward Propagation to get one_step_attention() with the layers\n",
        "def one_step_attention(a,s_prev):\n",
        "  '''\n",
        "  Execute attention:\n",
        "  a      : hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "  s_prev : previous hidden state of the (post-attention) LSTM, numpy-array of shape\n",
        "  context: context vector, input of the next (post-attention) LSTM cell\n",
        "  '''\n",
        "  s_prev=repeator(s_prev)\n",
        "  concat=concatenator([a,s_prev])\n",
        "  e=densor1(concat)\n",
        "  energies=densor2(e)\n",
        "  alphas=activator(energies)\n",
        "  context=dotor([alphas,a])\n",
        "  return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0iHf3JcrLEac"
      },
      "source": [
        "First getting activation from Bidirectional LSTM, then one_step_attention is performed Ty times to calculate context. Following that another layer of LSTM will predict y.\n",
        "\n",
        "\n",
        "<center>        \n",
        "<img src=\"attn_model.png\" style=\"width:500;height:500px;\"> \n",
        "</center>\n",
        "<caption><center> **Figure 2**: Neural machine translation with attention</center></caption>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8dmtrQSiIO6F",
        "colab": {}
      },
      "source": [
        "def model(Tx,Ty,n_a,n_s,human_vocab_size,machine_vocab_size):\n",
        "    \n",
        "    '''\n",
        "    n_a: hidden state size of the Bi-LSTM\n",
        "    n_s: hidden state size of the post-attention LSTM\n",
        "    model: Keras model instance\n",
        "    '''\n",
        "\n",
        "    X=Input(shape=(Tx,human_vocab_size))\n",
        "    s0=Input(shape=(n_s,),name='s0')   #  s0,c0 - initial hidden state for the decoder LSTM of shape (n_s,)\n",
        "    c0=Input(shape=(n_s,),name='c0')\n",
        "    s=s0\n",
        "    c=c0\n",
        " \n",
        "    outputs=[]\n",
        "    a = Bidirectional(LSTM(n_a,return_sequences=True))(X)\n",
        "    for t in range(Ty):\n",
        "      context=one_step_attention(a,s)\n",
        "      s,_,c=post_activation_LSTM_cell(context,initial_state=[s,c])\n",
        "      out=output_layer(s)\n",
        "      outputs.append(out)\n",
        "\n",
        "    model=Model(inputs=[X,s0,c0],outputs=outputs)\n",
        " \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Stc9_PSQJD0H",
        "colab": {}
      },
      "source": [
        "n_a = 32 \n",
        "n_s = 64\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
        "output_layer = Dense(len(machine_vocab), activation='softmax')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jhBAFWTgJulp",
        "colab": {}
      },
      "source": [
        "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MsJTovb4Jc4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "306777ea-3c88-4d20-e8e6-da39bd4101e3"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[0][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[1][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[2][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[3][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[4][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[5][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[6][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[7][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[8][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[9][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
            "                                                                 concatenate_1[1][0]              \n",
            "                                                                 concatenate_1[2][0]              \n",
            "                                                                 concatenate_1[3][0]              \n",
            "                                                                 concatenate_1[4][0]              \n",
            "                                                                 concatenate_1[5][0]              \n",
            "                                                                 concatenate_1[6][0]              \n",
            "                                                                 concatenate_1[7][0]              \n",
            "                                                                 concatenate_1[8][0]              \n",
            "                                                                 concatenate_1[9][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n",
            "                                                                 dense_1[1][0]                    \n",
            "                                                                 dense_1[2][0]                    \n",
            "                                                                 dense_1[3][0]                    \n",
            "                                                                 dense_1[4][0]                    \n",
            "                                                                 dense_1[5][0]                    \n",
            "                                                                 dense_1[6][0]                    \n",
            "                                                                 dense_1[7][0]                    \n",
            "                                                                 dense_1[8][0]                    \n",
            "                                                                 dense_1[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n",
            "                                                                 dense_2[1][0]                    \n",
            "                                                                 dense_2[2][0]                    \n",
            "                                                                 dense_2[3][0]                    \n",
            "                                                                 dense_2[4][0]                    \n",
            "                                                                 dense_2[5][0]                    \n",
            "                                                                 dense_2[6][0]                    \n",
            "                                                                 dense_2[7][0]                    \n",
            "                                                                 dense_2[8][0]                    \n",
            "                                                                 dense_2[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[1][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[2][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[3][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[4][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[5][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[6][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[7][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[8][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[9][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_1[1][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "                                                                 dot_1[2][0]                      \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[1][2]                     \n",
            "                                                                 dot_1[3][0]                      \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[2][2]                     \n",
            "                                                                 dot_1[4][0]                      \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[3][2]                     \n",
            "                                                                 dot_1[5][0]                      \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[4][2]                     \n",
            "                                                                 dot_1[6][0]                      \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[5][2]                     \n",
            "                                                                 dot_1[7][0]                      \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[6][2]                     \n",
            "                                                                 dot_1[8][0]                      \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[7][2]                     \n",
            "                                                                 dot_1[9][0]                      \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[8][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "==================================================================================================\n",
            "Total params: 52,960\n",
            "Trainable params: 52,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u6HaE97JKk60"
      },
      "source": [
        "Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bowVQ-IuKpyC",
        "colab": {}
      },
      "source": [
        "opt = Adam(lr=0.0005,beta_1=0.9,beta_2=0.999,decay=0.01)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "\n",
        "s0 = np.zeros((size,n_s))\n",
        "c0 = np.zeros((size,n_s))\n",
        "outputs = list(Y_1.swapaxes(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CAY5fqdLLIyX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f7c8d53a-5e8e-4da1-927c-e0a272f88194"
      },
      "source": [
        "# Input: X, s0, c0 of all training samples (m=10000, Tx=30)\n",
        "# Output: A list of 11 (m, Ty) elements. So: outputs[i][0], ..., outputs[i][Ty] represents the label characters of the i-th sample (X[i])\n",
        "\n",
        "history = model.fit([X_1,s0,c0],outputs,epochs=30,batch_size=100)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "30000/30000 [==============================] - 50s 2ms/step - loss: 18.4986 - dense_3_loss: 2.5811 - dense_3_accuracy: 0.5893 - dense_3_accuracy_1: 0.7018 - dense_3_accuracy_2: 0.3418 - dense_3_accuracy_3: 0.1241 - dense_3_accuracy_4: 0.6896 - dense_3_accuracy_5: 0.1530 - dense_3_accuracy_6: 0.0567 - dense_3_accuracy_7: 0.7113 - dense_3_accuracy_8: 0.1978 - dense_3_accuracy_9: 0.1079\n",
            "Epoch 2/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 15.1181 - dense_3_loss: 2.2917 - dense_3_accuracy: 0.9397 - dense_3_accuracy_1: 0.9351 - dense_3_accuracy_2: 0.5011 - dense_3_accuracy_3: 0.1584 - dense_3_accuracy_4: 0.8406 - dense_3_accuracy_5: 0.0955 - dense_3_accuracy_6: 0.0743 - dense_3_accuracy_7: 0.5294 - dense_3_accuracy_8: 0.3958 - dense_3_accuracy_9: 0.1953\n",
            "Epoch 3/30\n",
            "30000/30000 [==============================] - 43s 1ms/step - loss: 13.9807 - dense_3_loss: 2.1343 - dense_3_accuracy: 0.9636 - dense_3_accuracy_1: 0.9655 - dense_3_accuracy_2: 0.5891 - dense_3_accuracy_3: 0.1939 - dense_3_accuracy_4: 0.8912 - dense_3_accuracy_5: 0.0766 - dense_3_accuracy_6: 0.0770 - dense_3_accuracy_7: 0.4601 - dense_3_accuracy_8: 0.4525 - dense_3_accuracy_9: 0.2357\n",
            "Epoch 4/30\n",
            "30000/30000 [==============================] - 45s 1ms/step - loss: 13.3790 - dense_3_loss: 2.0519 - dense_3_accuracy: 0.9696 - dense_3_accuracy_1: 0.9725 - dense_3_accuracy_2: 0.6539 - dense_3_accuracy_3: 0.2254 - dense_3_accuracy_4: 0.9260 - dense_3_accuracy_5: 0.0921 - dense_3_accuracy_6: 0.0789 - dense_3_accuracy_7: 0.4290 - dense_3_accuracy_8: 0.4698 - dense_3_accuracy_9: 0.2592\n",
            "Epoch 5/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 12.9405 - dense_3_loss: 2.0075 - dense_3_accuracy: 0.9717 - dense_3_accuracy_1: 0.9755 - dense_3_accuracy_2: 0.6878 - dense_3_accuracy_3: 0.2477 - dense_3_accuracy_4: 0.9516 - dense_3_accuracy_5: 0.1570 - dense_3_accuracy_6: 0.0851 - dense_3_accuracy_7: 0.4340 - dense_3_accuracy_8: 0.4758 - dense_3_accuracy_9: 0.2694\n",
            "Epoch 6/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 12.5270 - dense_3_loss: 1.9849 - dense_3_accuracy: 0.9728 - dense_3_accuracy_1: 0.9778 - dense_3_accuracy_2: 0.7101 - dense_3_accuracy_3: 0.2638 - dense_3_accuracy_4: 0.9720 - dense_3_accuracy_5: 0.3253 - dense_3_accuracy_6: 0.0930 - dense_3_accuracy_7: 0.4947 - dense_3_accuracy_8: 0.4676 - dense_3_accuracy_9: 0.2727\n",
            "Epoch 7/30\n",
            "30000/30000 [==============================] - 43s 1ms/step - loss: 12.1087 - dense_3_loss: 1.9713 - dense_3_accuracy: 0.9741 - dense_3_accuracy_1: 0.9785 - dense_3_accuracy_2: 0.7243 - dense_3_accuracy_3: 0.2802 - dense_3_accuracy_4: 0.9840 - dense_3_accuracy_5: 0.5770 - dense_3_accuracy_6: 0.1007 - dense_3_accuracy_7: 0.6125 - dense_3_accuracy_8: 0.4602 - dense_3_accuracy_9: 0.2737\n",
            "Epoch 8/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 11.6486 - dense_3_loss: 1.9583 - dense_3_accuracy: 0.9752 - dense_3_accuracy_1: 0.9788 - dense_3_accuracy_2: 0.7410 - dense_3_accuracy_3: 0.2880 - dense_3_accuracy_4: 0.9899 - dense_3_accuracy_5: 0.7665 - dense_3_accuracy_6: 0.1081 - dense_3_accuracy_7: 0.7506 - dense_3_accuracy_8: 0.4704 - dense_3_accuracy_9: 0.2724\n",
            "Epoch 9/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 10.9404 - dense_3_loss: 1.9262 - dense_3_accuracy: 0.9758 - dense_3_accuracy_1: 0.9790 - dense_3_accuracy_2: 0.7570 - dense_3_accuracy_3: 0.2528 - dense_3_accuracy_4: 0.9891 - dense_3_accuracy_5: 0.8368 - dense_3_accuracy_6: 0.1443 - dense_3_accuracy_7: 0.8993 - dense_3_accuracy_8: 0.5468 - dense_3_accuracy_9: 0.2795\n",
            "Epoch 10/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 10.1580 - dense_3_loss: 1.8785 - dense_3_accuracy: 0.9754 - dense_3_accuracy_1: 0.9789 - dense_3_accuracy_2: 0.7519 - dense_3_accuracy_3: 0.2576 - dense_3_accuracy_4: 0.9943 - dense_3_accuracy_5: 0.8384 - dense_3_accuracy_6: 0.2014 - dense_3_accuracy_7: 0.9691 - dense_3_accuracy_8: 0.6175 - dense_3_accuracy_9: 0.2890\n",
            "Epoch 11/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 9.6686 - dense_3_loss: 1.8340 - dense_3_accuracy: 0.9754 - dense_3_accuracy_1: 0.9794 - dense_3_accuracy_2: 0.7486 - dense_3_accuracy_3: 0.2914 - dense_3_accuracy_4: 0.9975 - dense_3_accuracy_5: 0.8414 - dense_3_accuracy_6: 0.2380 - dense_3_accuracy_7: 0.9844 - dense_3_accuracy_8: 0.6417 - dense_3_accuracy_9: 0.3008\n",
            "Epoch 12/30\n",
            "30000/30000 [==============================] - 45s 1ms/step - loss: 9.3326 - dense_3_loss: 1.7999 - dense_3_accuracy: 0.9755 - dense_3_accuracy_1: 0.9791 - dense_3_accuracy_2: 0.7516 - dense_3_accuracy_3: 0.3176 - dense_3_accuracy_4: 0.9983 - dense_3_accuracy_5: 0.8429 - dense_3_accuracy_6: 0.2661 - dense_3_accuracy_7: 0.9893 - dense_3_accuracy_8: 0.6482 - dense_3_accuracy_9: 0.3110\n",
            "Epoch 13/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 9.0764 - dense_3_loss: 1.7749 - dense_3_accuracy: 0.9761 - dense_3_accuracy_1: 0.9793 - dense_3_accuracy_2: 0.7572 - dense_3_accuracy_3: 0.3388 - dense_3_accuracy_4: 0.9987 - dense_3_accuracy_5: 0.8471 - dense_3_accuracy_6: 0.2896 - dense_3_accuracy_7: 0.9922 - dense_3_accuracy_8: 0.6530 - dense_3_accuracy_9: 0.3183\n",
            "Epoch 14/30\n",
            "30000/30000 [==============================] - 43s 1ms/step - loss: 8.8678 - dense_3_loss: 1.7562 - dense_3_accuracy: 0.9762 - dense_3_accuracy_1: 0.9795 - dense_3_accuracy_2: 0.7603 - dense_3_accuracy_3: 0.3554 - dense_3_accuracy_4: 0.9989 - dense_3_accuracy_5: 0.8492 - dense_3_accuracy_6: 0.3056 - dense_3_accuracy_7: 0.9939 - dense_3_accuracy_8: 0.6555 - dense_3_accuracy_9: 0.3243\n",
            "Epoch 15/30\n",
            "30000/30000 [==============================] - 43s 1ms/step - loss: 8.6899 - dense_3_loss: 1.7413 - dense_3_accuracy: 0.9768 - dense_3_accuracy_1: 0.9795 - dense_3_accuracy_2: 0.7647 - dense_3_accuracy_3: 0.3691 - dense_3_accuracy_4: 0.9991 - dense_3_accuracy_5: 0.8519 - dense_3_accuracy_6: 0.3218 - dense_3_accuracy_7: 0.9953 - dense_3_accuracy_8: 0.6581 - dense_3_accuracy_9: 0.3286\n",
            "Epoch 16/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 8.5394 - dense_3_loss: 1.7285 - dense_3_accuracy: 0.9769 - dense_3_accuracy_1: 0.9795 - dense_3_accuracy_2: 0.7687 - dense_3_accuracy_3: 0.3778 - dense_3_accuracy_4: 0.9992 - dense_3_accuracy_5: 0.8537 - dense_3_accuracy_6: 0.3331 - dense_3_accuracy_7: 0.9965 - dense_3_accuracy_8: 0.6592 - dense_3_accuracy_9: 0.3330\n",
            "Epoch 17/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 8.4095 - dense_3_loss: 1.7186 - dense_3_accuracy: 0.9772 - dense_3_accuracy_1: 0.9797 - dense_3_accuracy_2: 0.7742 - dense_3_accuracy_3: 0.3858 - dense_3_accuracy_4: 0.9993 - dense_3_accuracy_5: 0.8554 - dense_3_accuracy_6: 0.3410 - dense_3_accuracy_7: 0.9971 - dense_3_accuracy_8: 0.6617 - dense_3_accuracy_9: 0.3356\n",
            "Epoch 18/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 8.2945 - dense_3_loss: 1.7092 - dense_3_accuracy: 0.9775 - dense_3_accuracy_1: 0.9796 - dense_3_accuracy_2: 0.7791 - dense_3_accuracy_3: 0.3924 - dense_3_accuracy_4: 0.9993 - dense_3_accuracy_5: 0.8569 - dense_3_accuracy_6: 0.3479 - dense_3_accuracy_7: 0.9977 - dense_3_accuracy_8: 0.6650 - dense_3_accuracy_9: 0.3384\n",
            "Epoch 19/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 8.1912 - dense_3_loss: 1.7018 - dense_3_accuracy: 0.9774 - dense_3_accuracy_1: 0.9796 - dense_3_accuracy_2: 0.7825 - dense_3_accuracy_3: 0.3988 - dense_3_accuracy_4: 0.9993 - dense_3_accuracy_5: 0.8594 - dense_3_accuracy_6: 0.3551 - dense_3_accuracy_7: 0.9981 - dense_3_accuracy_8: 0.6662 - dense_3_accuracy_9: 0.3415\n",
            "Epoch 20/30\n",
            "30000/30000 [==============================] - 43s 1ms/step - loss: 8.0971 - dense_3_loss: 1.6947 - dense_3_accuracy: 0.9780 - dense_3_accuracy_1: 0.9799 - dense_3_accuracy_2: 0.7864 - dense_3_accuracy_3: 0.4031 - dense_3_accuracy_4: 0.9993 - dense_3_accuracy_5: 0.8605 - dense_3_accuracy_6: 0.3615 - dense_3_accuracy_7: 0.9984 - dense_3_accuracy_8: 0.6671 - dense_3_accuracy_9: 0.3439\n",
            "Epoch 21/30\n",
            "30000/30000 [==============================] - 45s 1ms/step - loss: 8.0116 - dense_3_loss: 1.6893 - dense_3_accuracy: 0.9780 - dense_3_accuracy_1: 0.9802 - dense_3_accuracy_2: 0.7896 - dense_3_accuracy_3: 0.4077 - dense_3_accuracy_4: 0.9995 - dense_3_accuracy_5: 0.8625 - dense_3_accuracy_6: 0.3672 - dense_3_accuracy_7: 0.9986 - dense_3_accuracy_8: 0.6694 - dense_3_accuracy_9: 0.3445\n",
            "Epoch 22/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 7.9325 - dense_3_loss: 1.6830 - dense_3_accuracy: 0.9784 - dense_3_accuracy_1: 0.9800 - dense_3_accuracy_2: 0.7922 - dense_3_accuracy_3: 0.4116 - dense_3_accuracy_4: 0.9995 - dense_3_accuracy_5: 0.8641 - dense_3_accuracy_6: 0.3725 - dense_3_accuracy_7: 0.9987 - dense_3_accuracy_8: 0.6705 - dense_3_accuracy_9: 0.3481\n",
            "Epoch 23/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 7.8589 - dense_3_loss: 1.6777 - dense_3_accuracy: 0.9786 - dense_3_accuracy_1: 0.9802 - dense_3_accuracy_2: 0.7945 - dense_3_accuracy_3: 0.4156 - dense_3_accuracy_4: 0.9995 - dense_3_accuracy_5: 0.8654 - dense_3_accuracy_6: 0.3777 - dense_3_accuracy_7: 0.9989 - dense_3_accuracy_8: 0.6731 - dense_3_accuracy_9: 0.3482\n",
            "Epoch 24/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 7.7904 - dense_3_loss: 1.6729 - dense_3_accuracy: 0.9787 - dense_3_accuracy_1: 0.9803 - dense_3_accuracy_2: 0.7969 - dense_3_accuracy_3: 0.4190 - dense_3_accuracy_4: 0.9996 - dense_3_accuracy_5: 0.8670 - dense_3_accuracy_6: 0.3820 - dense_3_accuracy_7: 0.9991 - dense_3_accuracy_8: 0.6756 - dense_3_accuracy_9: 0.3500\n",
            "Epoch 25/30\n",
            "30000/30000 [==============================] - 43s 1ms/step - loss: 7.7263 - dense_3_loss: 1.6684 - dense_3_accuracy: 0.9788 - dense_3_accuracy_1: 0.9803 - dense_3_accuracy_2: 0.7986 - dense_3_accuracy_3: 0.4231 - dense_3_accuracy_4: 0.9996 - dense_3_accuracy_5: 0.8680 - dense_3_accuracy_6: 0.3869 - dense_3_accuracy_7: 0.9992 - dense_3_accuracy_8: 0.6770 - dense_3_accuracy_9: 0.3514\n",
            "Epoch 26/30\n",
            "30000/30000 [==============================] - 45s 2ms/step - loss: 7.6657 - dense_3_loss: 1.6644 - dense_3_accuracy: 0.9789 - dense_3_accuracy_1: 0.9803 - dense_3_accuracy_2: 0.7997 - dense_3_accuracy_3: 0.4268 - dense_3_accuracy_4: 0.9996 - dense_3_accuracy_5: 0.8706 - dense_3_accuracy_6: 0.3907 - dense_3_accuracy_7: 0.9992 - dense_3_accuracy_8: 0.6791 - dense_3_accuracy_9: 0.3526\n",
            "Epoch 27/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 7.6089 - dense_3_loss: 1.6603 - dense_3_accuracy: 0.9793 - dense_3_accuracy_1: 0.9804 - dense_3_accuracy_2: 0.8018 - dense_3_accuracy_3: 0.4289 - dense_3_accuracy_4: 0.9996 - dense_3_accuracy_5: 0.8713 - dense_3_accuracy_6: 0.3952 - dense_3_accuracy_7: 0.9993 - dense_3_accuracy_8: 0.6810 - dense_3_accuracy_9: 0.3534\n",
            "Epoch 28/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 7.5553 - dense_3_loss: 1.6565 - dense_3_accuracy: 0.9792 - dense_3_accuracy_1: 0.9803 - dense_3_accuracy_2: 0.8028 - dense_3_accuracy_3: 0.4328 - dense_3_accuracy_4: 0.9996 - dense_3_accuracy_5: 0.8727 - dense_3_accuracy_6: 0.3991 - dense_3_accuracy_7: 0.9995 - dense_3_accuracy_8: 0.6813 - dense_3_accuracy_9: 0.3547\n",
            "Epoch 29/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 7.5046 - dense_3_loss: 1.6526 - dense_3_accuracy: 0.9794 - dense_3_accuracy_1: 0.9806 - dense_3_accuracy_2: 0.8038 - dense_3_accuracy_3: 0.4344 - dense_3_accuracy_4: 0.9996 - dense_3_accuracy_5: 0.8748 - dense_3_accuracy_6: 0.4032 - dense_3_accuracy_7: 0.9995 - dense_3_accuracy_8: 0.6838 - dense_3_accuracy_9: 0.3553\n",
            "Epoch 30/30\n",
            "30000/30000 [==============================] - 44s 1ms/step - loss: 7.4566 - dense_3_loss: 1.6494 - dense_3_accuracy: 0.9794 - dense_3_accuracy_1: 0.9806 - dense_3_accuracy_2: 0.8057 - dense_3_accuracy_3: 0.4367 - dense_3_accuracy_4: 0.9996 - dense_3_accuracy_5: 0.8767 - dense_3_accuracy_6: 0.4064 - dense_3_accuracy_7: 0.9996 - dense_3_accuracy_8: 0.6846 - dense_3_accuracy_9: 0.3568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqNIWravNp6U",
        "colab": {}
      },
      "source": [
        "model.save('NMT_date_conversion_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCLJUkgWtqf_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "4656a15a-f68e-41cc-e490-8bcc6ab7fee5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_C54r5P2P7R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "d3cd342b-d273-4b37-f8e1-2142421ba38e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(history.history['dense_3_accuracy'])\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Accuracy'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q3wDiae1HSfi"
      },
      "source": [
        "## Next Step\n",
        "\n",
        "- Model can be trained on higher size of data \n",
        "- Different parameters can be tried like learning rate, units etc.\n",
        "- Different optimization function can be tried like LeakyRelu etc. \n",
        "- We can play more around the architechure adding more layers. \n",
        "\n"
      ]
    }
  ]
}